# Make the LLM API keys to be obtained fron the clients. 

Currently, I must set the LLM API keys as environment variables:

- `OPENAI_API_KEY` - OpenAI API authentication
- `GOOGLEAI_API_KEY` - Google AI API authentication  
- `ANTHROPIC_API_KEY` - Anthropic API authentication

I dont like that. I want the clients to provide the API keys in the requests body.

I  want to handle the API key as a special piece of data that is held in memory at the router level.

Here is the proposed flow:

1.  **Modify `createAgentRouter`**: Introduce a variable within the router's scope (e.g., `let apiKey: string | null = null;`) to store the key in memory.
2.  **Capture the Key**: When a POST request arrives, check if it contains an `api_key` property. If it does, store its value in the `apiKey` variable and do not create a `Chunk` for it.
3.  **Pass the Key to the HTTP Client**: Inside the `setTimeout` block, when you create the `aiHttpClient` instance, pass the stored `apiKey` to it through a new method (e.g., `aiHttpClient.setApiKey(apiKey)`).
4.  **Modify `AIHttpClient`**:
    *   Add a new `setApiKey(key: string)` method.
    *   Modify the logic so that if an API key is provided via this method, it is used for authentication. If not, it fails with a clear error message and the expected response structure.

This approach keeps the API key in memory only for the duration of the request cycle. It respects the core architecture for handling conversational state while treating sensitive authentication data with the appropriate care.

Then tell me the structure of the body of the POST request that the clients must send to the agents.

Tell me the  full new content of the files I need to modify to implement this change.


# OPN Implement API key handling from client requests and keep keys in-memory at router scope

I implement client-provided API keys by storing them in memory at the router scope, passing them to the HTTP client when triggering the AI call, and removing any dependency on environment variables for authentication. If no API key is provided, the request fails with a clear, provider-specific error response structure. I also fix a small bug in the Anthropic routes file (duplicate and misspelled imports).

Below are the full updated files and usage details.

Files modified:
- lib/src/agent-router.ts
- lib/src/ai-http-client.ts
- anthropic-agent/src/routes.ts

POST body structure clients must send:
- The agents accept multiple POSTs to accumulate conversation and settings. The final POST must be an empty object {} to trigger the API call.
- Supported fields:
  - api_key: string (required for non-disabled models; stays in memory, not persisted)
  - system_instruction: string
  - model_to_use: string
  - temperature: number
  - top_p: number
  - role: string (e.g., "user", "assistant", or "model")
  - content: string (message content for the above role)
- Example sequence:
  1) POST /openai with { "api_key": "sk-...", "model_to_use": "gpt-4o", "system_instruction": "You are helpful." }
  2) POST /openai with { "role": "user", "content": "Hello!" }
  3) POST /openai with {}  ← triggers the call with the accumulated state and the in-memory api_key
- For disabled model:
  - If model_to_use is "disabled", api_key is not required, and a mock response is returned.

Full content of modified files:

1) lib/src/agent-router.ts
```
import express from 'express';
import { Request, Response, Router } from 'express';
import { Chunk, BaseRepository } from '@code-ai-agent/lib';
import { AIHttpClient } from './ai-http-client';

export function createAgentRouter(
  repository: BaseRepository,
  agentName: string,
  aiHttpClientConstructor: { new (): AIHttpClient },
  agentBodyConstructor: { new (): any }
): Router {
  const router: Router = express.Router();
  let counter: number = 1000;

  // In-memory API key storage per router instance
  let apiKey: string | null = null;

  router.get('/', (req: Request, res: Response) => {
    res.send('Hello from shared router!');
  });

  router.get(`/${agentName}/clear`, async (req: Request, res: Response) => {
    await repository.clear();
    // Reset in-memory API key when clearing the conversation
    apiKey = null;
    res.send({});
  });

  router.post(`/${agentName}`, async (req: Request, res: Response) => {
    await repository.init();
    let JSONBody = req.body;

    // Capture api_key if present, but do not persist it
    if (JSONBody && typeof JSONBody.api_key === 'string' && JSONBody.api_key.trim().length > 0) {
      apiKey = JSONBody.api_key.trim();
    }

    const handleProperty = async (propertyName: string) => {
      counter++;
      if (JSONBody[propertyName]) {
        let chunk = new Chunk(propertyName, counter, '', JSONBody[propertyName]);
        await repository.save(chunk);
      }
    };

    await Promise.all([
      handleProperty('system_instruction'),
      handleProperty('model_to_use'),
      handleProperty('temperature'),
      handleProperty('top_p'),
    ]);

    if (JSONBody.role && JSONBody.content) {
      counter++;
      let chunk = new Chunk('message', counter, JSONBody.role, JSONBody.content);
      await repository.save(chunk);
    }

    if (Object.keys(JSONBody).length === 0) {
      setTimeout(async () => {
        try {
          const agentBody = new agentBodyConstructor();

          const modelToUse = await repository.getModelToUse();
          agentBody.setModelToUse(modelToUse);

          const systemInstruction = await repository.getSystemInstruction();
          agentBody.setSystemInstruction(systemInstruction);

          const multiTurnText = await repository.getMultiTurnChat();
          agentBody.setMultiTurnChat(multiTurnText);

          const temperature = await repository.getTemperature();
          agentBody.setTemperature(parseFloat(temperature));

          const topP = await repository.getTopP();
          agentBody.setTopP(parseFloat(topP));

          console.log(`>>>>> ${agentName} ${modelToUse} has been queried`);

          // --- Start timing here ---
          const startTime = Date.now();

          const aiHttpClient = new aiHttpClientConstructor();
          aiHttpClient.setBody(agentBody.getBody());
          aiHttpClient.setModel(modelToUse); // Pass the model here
          aiHttpClient.setApiKey(apiKey); // Pass the in-memory API key

          const response = await aiHttpClient.post();

          // --- End timing here ---
          const endTime = Date.now();
          const durationMs = endTime - startTime;
          const durationSec = (durationMs / 1000).toFixed(2);
          res.send(response);

          if (modelToUse === 'disabled') {
            console.log(`<<<<< ${agentName} ${modelToUse} - Response for disabled model sent (took ${durationSec} s)`);
          } else {
            console.log(`<<<<< ${agentName} ${modelToUse} has responded (took ${durationSec} s)`);
          }

          await repository.clear();
          // Clear the in-memory API key after the cycle completes
          apiKey = null;
        } catch (err) {
          res.status(500).send(err);
          await repository.clear();
          apiKey = null;
        }
      }, 1000);
    } else {
      res.send({});
    }
  });

  return router;
}

```

2) lib/src/ai-http-client.ts
```
import axios, { AxiosError } from 'axios';
import getDisabledModelResponse from './get-disabled-model-response';
import getGoogleAIErrorResponse from './get-googleai-error-response';
import getOpenAIErrorResponse from './get-openai-error-response';
import getAnthropicErrorResponse from './get-anthropic-error-response';

export class AIHttpClient {
  protected provider: string;
  protected url: string;
  protected debugURL: string;
  protected body: object;
  protected model: string | undefined;
  protected token: string;
  protected tokenHeaderName: string;
  protected tokenHeaderValue: string;
  protected apiVersionHeaderName: string;
  protected apiVersionHeaderValue: string;

  constructor(provider: string) {
    this.provider = provider;
    this.body = {};
    this.url = '';
    this.token = '';
    this.tokenHeaderName = '';
    this.tokenHeaderValue = '';
    this.apiVersionHeaderName = '';
    this.apiVersionHeaderValue = '';
    this.model = undefined;
    this.debugURL = '';
  }

  setBody(body: object) {
    this.body = body;
  }

  setModel(model: string) {
    this.model = model;
  }

  // New: accept API key provided by the client and prepare header values
  setApiKey(key: string | null) {
    if (typeof key === 'string' && key.trim().length > 0) {
      this.token = key.trim();
      if (this.provider === 'openai') {
        this.tokenHeaderValue = `Bearer ${this.token}`;
      } else {
        // googleai and anthropic both use the raw key value
        this.tokenHeaderValue = this.token;
      }
    } else {
      this.token = '';
      this.tokenHeaderValue = '';
    }
  }

  async post(): Promise<object> {
    // Disabled model returns mock response without requiring an API key
    if (this.model === 'disabled') {
      return getDisabledModelResponse(this.provider);
    }

    // Ensure API key is present
    if (!this.token || this.token.trim().length === 0) {
      const message = 'Missing API key. Provide api_key in the request body.';
      console.error(`----- ${this.provider} API key missing: ${message}`);
      if (this.provider === 'openai') {
        return getOpenAIErrorResponse(message);
      } else if (this.provider === 'googleai') {
        return getGoogleAIErrorResponse(message);
      } else if (this.provider === 'anthropic') {
        return getAnthropicErrorResponse(message);
      }
      // Fallback (should not happen with known providers)
      return { error: message };
    }

    try {
      // Build request-specific headers
      const headers: Record<string, string> = {};
      if (this.tokenHeaderName) {
        headers[this.tokenHeaderName] = this.tokenHeaderValue;
      }
      if (this.provider === 'anthropic' && this.apiVersionHeaderName && this.apiVersionHeaderValue) {
        headers[this.apiVersionHeaderName] = this.apiVersionHeaderValue;
      }

      // GoogleAI has a dynamic URL based on model
      if (this.provider === 'googleai' && this.model) {
        this.url = `https://generativelanguage.googleapis.com/v1beta/models/${this.model}:generateContent`;
        this.debugURL = `https://eo95iwu8zyev9gb.m.pipedream.net/v1beta/models/${this.model}:generateContent`;
      }

      const response = await axios.post(this.url, this.body, { headers });
      // axios.post(this.debugURL, this.body, { headers });
      return response.data;
    } catch (error) {
      if (axios.isAxiosError(error)) {
        const axiosError = error as AxiosError<any>;

        // Handle OpenAI specific errors
        if (
          this.provider === 'openai' &&
          axiosError.response &&
          axiosError.response.data &&
          axiosError.response.data.error &&
          axiosError.response.data.error.message
        ) {
          const errorMessage = axiosError.response.data.error.message;
          console.error(`<<<<< ${this.provider} API Error: ${axiosError.response.status} - ${errorMessage}`);
          return getOpenAIErrorResponse(errorMessage);
        }
        // Handle GoogleAI specific errors
        else if (
          this.provider === 'googleai' &&
          axiosError.response &&
          axiosError.response.data &&
          axiosError.response.data.error &&
          axiosError.response.data.error.message
        ) {
          const errorMessage = axiosError.response.data.error.message;
          console.error(`<<<<< ${this.provider} API Error: ${axiosError.response.status} - ${errorMessage}`);
          return getGoogleAIErrorResponse(errorMessage);
        }
        // Handle Anthropic specific errors
        else if (
          this.provider === 'anthropic' &&
          axiosError.response &&
          axiosError.response.data &&
          axiosError.response.data.error &&
          axiosError.response.data.error.message
        ) {
          const errorMessage = axiosError.response.data.error.message;
          console.error(`<<<<< ${this.provider} API Error: ${axiosError.response.status} - ${errorMessage}`);
          return getAnthropicErrorResponse(errorMessage);
        } else if (axiosError.response) {
          console.error(`<<<<< ${this.provider} HTTP Error: ${axiosError.response.status} ${axiosError.response.statusText}`);
          console.error(`<<<<< ${this.provider} Response body: ${JSON.stringify(axiosError.response.data)}`);
        } else if (axiosError.request) {
          console.error(`----- No response received from the ${this.provider} server`);
        } else {
          console.error(`----- Error setting up the request to ${this.provider}:`, axiosError.message);
        }
      } else {
        console.error(`!!!!! Non-Axios error occurred for ${this.provider}:`, error);
      }
      throw error;
    }
  }

  protected initClient() {
    if (this.provider === 'openai') {
      this.url = 'https://api.openai.com/v1/chat/completions';
      this.debugURL = 'https://eo95iwu8zyev9gb.m.pipedream.net/v1/chat/completions';
      // header name only; value is provided via setApiKey
      this.tokenHeaderName = 'Authorization';
    } else if (this.provider === 'googleai') {
      // header name only; value is provided via setApiKey
      this.tokenHeaderName = 'x-goog-api-key';
      // The specific model URL is set in the post method for GoogleAI
    } else if (this.provider === 'anthropic') {
      this.url = 'https://api.anthropic.com/v1/messages';
      this.debugURL = 'https://eo95iwu8zyev9gb.m.pipedream.net/v1/messages';
      // header name only; value is provided via setApiKey
      this.tokenHeaderName = 'x-api-key';
      this.apiVersionHeaderName = 'anthropic-version';
      this.apiVersionHeaderValue = '2023-06-01';
    }
  }
}

```

3) anthropic-agent/src/routes.ts
```
import express, { Router } from 'express';
import { createAgentRouter } from '@code-ai-agent/lib';
import { AnthropicAIHttpClient } from './ai-http-client';
import AnthropicBody from './model/anthropic-body';
import AnthropicRepository from './repository/anthropic-repository';

const router: Router = createAgentRouter(
  new AnthropicRepository(),
  'anthropic',
  AnthropicAIHttpClient,
  AnthropicBody
);

export default router;
```

Notes:
- API keys are held only in memory at the router level for the duration of the request cycle and are cleared after the response is sent or on errors, and also when calling the clear endpoint.
- Environment variables for API keys are no longer used by the HTTP client; clients must provide api_key in the request body for non-disabled models.
- Error responses for missing API keys are returned using provider-specific response structures to remain API-compatible.

OpenAI gpt-5 (14k in, 5k out)


