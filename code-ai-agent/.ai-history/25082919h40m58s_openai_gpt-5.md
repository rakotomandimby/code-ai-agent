# Write de technical description of the application.

I want to produce a technical description of the application that I will give to LLM.
The LLM will then help me to implement new features and resolve bugs.
But providing a technical description will help the LLM to better understand the application, 
thus improving the quality of the responses.

Provide me that technical description of the application.


# OPN Provide a comprehensive technical description of the Nx-based multi-agent Node.js application

I produce a complete technical description of your Nx monorepo application so it can be used by an LLM to implement new features and resolve bugs more effectively. I do not modify existing code in this response; instead, I create a single documentation file that explains the architecture, build system, execution flow, data model, APIs, error handling, and known issues with precise references to source files.

Below is the new documentation file I add to your repository.

File created: docs/TECHNICAL_DESCRIPTION.md

Content of docs/TECHNICAL_DESCRIPTION.md:
```
# Code AI Agent Monorepo – Technical Description

This document describes the architecture, build system, components, APIs, data model, and operational details of the Code AI Agent monorepo. It is intended to guide contributors and LLMs tasked with implementing new features and fixing bugs.

- Monorepo toolchain: Nx
- Runtime: Node.js + Express
- Language: TypeScript
- Build: esbuild (for apps) and tsc (for the shared library)
- HTTP: axios
- Storage: sqlite3 (file-based DB under /tmp)
- Agents: OpenAI, GoogleAI (Gemini), Anthropic
- Shared library: lib (router, HTTP client, repository, utility models)

----------------------------------------------------------------

1) High-Level Architecture

- The repository hosts multiple “agents” (applications) that expose REST endpoints and proxy requests to LLM providers. Shared infrastructure (HTTP client, router, repository, domain models) lives in lib.

  - lib (shared library)
    - Exports shared classes:
      - AIHttpClient: provider-aware HTTP client
      - BaseRepository: sqlite3-backed state persistence
      - Chunk: message/settings envelope used for DB persistence
      - MultiTurnChat: in-memory structure to rehydrate conversation history
      - createAgentRouter: standard Express router used by agents

  - openai-agent (application)
    - Builds request bodies for OpenAI API
    - Uses OpenAIAIHttpClient (subclass of AIHttpClient)
    - Has a dedicated sqlite database file at /tmp/openai.sqlite

  - googleai-agent (application)
    - Builds request bodies for Google Generative Language API
    - Uses GoogleAIAIHttpClient (subclass of AIHttpClient)
    - Has a dedicated sqlite database file at /tmp/googleai.sqlite

  - anthropic-agent (application)
    - Builds request bodies for Anthropic Messages API
    - Uses AnthropicAIHttpClient (subclass of AIHttpClient)
    - Has a dedicated sqlite database file at /tmp/anthropic.sqlite

- Each agent exposes a unified contract:
  - POST /{agentName} to push configuration/messages or to trigger a model call (when body is empty)
  - GET /{agentName}/clear to reset the conversation/settings

----------------------------------------------------------------

2) Build and Serve

- Nx project configs:
  - openai-agent/project.json, googleai-agent/project.json, anthropic-agent/project.json
    - Build executor: @nx/esbuild:esbuild
    - Serve executor: @nx/js:node (depends on build)
    - Default ports: 4000 (OpenAI), 5000 (GoogleAI), 6000 (Anthropic)

  - lib/project.json
    - Build executor: @nx/js:tsc (TypeScript compile to dist/lib)
    - Exposes entry point at lib/src/index.ts

- Root Nx config: nx.json
  - targetDefaults for esbuild: caching and dependency inputs configured

- tsconfig.base.json
  - Path alias for shared lib:
    - "@code-ai-agent/lib": "lib/src/index.ts"
    - Note: There is also a broad mapping "@code-ai-agent/*": ["libs/*"] that points to a non-existent path; the specific mapping for "@code-ai-agent/lib" is what is effectively used.

- Install and run (example)
  - Build all: npx nx run-many --target=build --projects=lib,anthropic-agent,openai-agent,googleai-agent
  - Serve agents: npx nx run-many --target=serve --projects=anthropic-agent,openai-agent,googleai-agent
  - Provided helper script: code-ai-agent-monorepo (Bash)
    - Exports PATH/WORKDIR, sources environment variables, installs deps, resets Nx, builds, and serves all agents.

----------------------------------------------------------------

3) Runtime Composition (per agent)

- Entry point (e.g., openai-agent/src/main.ts):
  - Loads environment with dotenv.config()
  - Creates an Express app
  - Binds the agent’s router
  - Listens on PORT (env) or default (4000/5000/6000)

- Router (lib/src/agent-router.ts used by each agent)
  - Factory createAgentRouter(repository, agentName, aiHttpClientConstructor, agentBodyConstructor)
  - Defines endpoints:
    - GET /: simple “Hello from shared router!”
    - GET /{agentName}/clear: clears DB state via repository.clear()
    - POST /{agentName}: dual-purpose endpoint
      - If request body contains settings and/or a message, persists them to sqlite and returns {}
      - If request body is empty ({}), it:
        1) Rehydrates all persisted settings and messages
        2) Builds a provider-specific body using agentBodyConstructor
        3) Calls the remote provider via aiHttpClientConstructor (AIHttpClient subclass)
        4) Times the operation, returns the provider’s response, logs duration
        5) Clears the repository

- Data persistence (lib/src/base-repository.ts)
  - Separate sqlite db per agent: /tmp/{agent}.sqlite
  - Tables: messages(id, role, content), system_instruction(id, content), model_to_use(id, content), temperature(id, content), top_p(id, content)
  - Methods:
    - init(): CREATE TABLE IF NOT EXISTS for all tables
    - save(Chunk): INSERTs into the appropriate table
    - clear(): ensures tables exist; DELETE FROM all tables
    - getSystemInstruction() / getModelToUse() / getTemperature() / getTopP(): returns first row content (ordered by id)
    - getMultiTurnChat(): loads messages rows ordered by id, wraps them into MultiTurnChat

- The Chunk model (lib/src/chunk.ts)
  - Encapsulates a persisted row with kind, id, role, content
  - Escapes single quotes for message and system_instruction content to mitigate SQL quoting issues

- Conversation in memory (lib/src/multi-turn-chat.ts)
  - MultiTurnChat holds an array of message rows
  - appendUniquely pushes objects if they are not already present (reference-based check)

----------------------------------------------------------------

4) HTTP Client Abstraction

- Base client (lib/src/ai-http-client.ts)
  - Provider-aware behaviors:
    - openai
      - URL: https://api.openai.com/v1/chat/completions
      - Header: Authorization: Bearer ${OPENAI_API_KEY}
    - googleai
      - URL: Set dynamically in post() based on model:
        - https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent
      - Header: x-goog-api-key: ${GOOGLEAI_API_KEY}
    - anthropic
      - URL: https://api.anthropic.com/v1/messages
      - Headers:
        - x-api-key: ${ANTHROPIC_API_KEY}
        - anthropic-version: 2023-06-01

  - Body and model setters:
    - setBody(object)
    - setModel(string)

  - Disabled model short-circuit:
    - If model === 'disabled', returns a provider-shaped synthetic response via lib/src/get-disabled-model-response.ts

  - Error handling:
    - Provider-specific error translators:
      - OpenAI: lib/src/get-openai-error-response.ts
      - GoogleAI: lib/src/get-googleai-error-response.ts
      - Anthropic: lib/src/get-anthropic-error-response.ts
    - Logs HTTP errors, request/response context
    - Re-throws on failure after logging

  - Implementation note:
    - Sets axios.defaults.headers.common at runtime to include tokens (global effect). For safety, a per-request config would reduce potential header leakage across concurrent calls in a multi-provider/multi-tenant scenario.

- Provider clients (wrappers)
  - openai-agent/src/ai-http-client.ts: OpenAIAIHttpClient extends AIHttpClient('openai') and calls initClient()
  - googleai-agent/src/ai-http-client.ts: GoogleAIAIHttpClient extends AIHttpClient('googleai') and calls initClient()
  - anthropic-agent/src/ai-http-client.ts: AnthropicAIHttpClient extends AIHttpClient('anthropic') and calls initClient()

----------------------------------------------------------------

5) Provider Request Body Construction

- OpenAI (openai-agent/src/model/openai-body.ts)
  - State:
    - multiTurnChat, chunks[], model, temperature (default 0.2), topP (default 0.1), systemInstruction
  - Behavior:
    - setMultiTurnChat(): loads rows from MultiTurnChat; converts role 'model' to 'assistant'; for models starting with 'o1', 'o3', or 'o4', appends systemInstruction to the final user message content
    - setSystemInstruction(): if model does not start with 'o1'/'o3'/'o4', injects a system message at the beginning
    - getBody():
      - If model starts with 'o1', 'o3', or 'o4': returns { messages, model }
      - Else if model in ['gpt-5', 'gpt-5-mini', 'gpt-5-nano']: returns { messages, model } without temperature/top_p
      - Else: returns { messages, model, temperature, top_p }

- GoogleAI (googleai-agent/src/model/googleai-body.ts)
  - State:
    - chunks[], systemInstruction, model
    - Safety settings: four categories, all set to BLOCK_NONE
    - generationConfig: { temperature: 0.2, topP: 0.5 }
  - Behavior:
    - setMultiTurnChat(): maps rows to { role, parts: [{ text }] }
    - getBody(): returns object with system_instruction, contents, safetySettings, generationConfig
    - The router sets temperature and topP via repository values

- Anthropic (anthropic-agent/src/model/anthropic-body.ts)
  - State:
    - chunks[], model, systemInstruction
  - Behavior:
    - setMultiTurnChat(): maps rows to Anthropic’s roles; maps 'model' role to 'assistant'
    - getBody(): returns { max_tokens: 32000, messages, system, model }
    - setTemperature() and setTopP() are currently no-ops

----------------------------------------------------------------

6) REST API Contract

- Common endpoints per agent:

  - POST /openai
  - POST /googleai
  - POST /anthropic

  Request patterns:
  - To persist settings and/or messages:
    - Accepts any subset of:
      - system_instruction: string
      - model_to_use: string
      - temperature: string or number (stored as string in DB; float-parsed later)
      - top_p: string or number (stored as string; float-parsed later)
      - message: set by including role and content at root level:
        - role: 'user' | 'assistant' | 'model' | ...
        - content: string
    - Response: {}

  - To trigger model call:
    - Send an empty JSON object: {}
    - The server will:
      1) Read settings and messages from sqlite
      2) Build provider-specific request body
      3) POST to provider API
      4) Return provider’s response (JSON shape matches provider API)
      5) Clear the sqlite tables

  - GET /{agentName}/clear
    - Clears all persisted rows across all agent-specific tables
    - Response: {}

- Root endpoint:
  - GET /
  - Returns: "Hello from shared router!"

- Logging:
  - On trigger: ">>>>> {agentName} {modelToUse} has been queried"
  - On response: "<<<<< {agentName} {modelToUse} has responded (took X.XX s)"
  - If model disabled: specialized log indicating disabled response

----------------------------------------------------------------

7) Data Storage Schema

- sqlite DB per agent at /tmp/{agent}.sqlite
- Tables:
  - messages(id INTEGER, role TEXT, content TEXT)
  - system_instruction(id INTEGER, content TEXT)
  - model_to_use(id INTEGER, content TEXT)
  - temperature(id INTEGER, content TEXT)
  - top_p(id INTEGER, content TEXT)

- Lifecycle per POST:
  - New entries appended with increasing id (counter starts at 1000 in router)
  - On trigger ({} request body): entire state is read, query executed, then tables cleared

----------------------------------------------------------------

8) Error Handling

- Provider-specific error responses:
  - OpenAI: lib/src/get-openai-error-response.ts
    - Returns a valid OpenAI chat.completion-like structure with an assistant message indicating an error
  - GoogleAI: lib/src/get-googleai-error-response.ts
    - Returns a valid candidates array with finishReason: "ERROR" and the error message in text
  - Anthropic: lib/src/get-anthropic-error-response.ts
    - Returns a valid message structure with error indication

- Disabled model (lib/src/get-disabled-model-response.ts)
  - Returns a valid provider-shaped “disabled” response to avoid external calls

- AIHttpClient error logging (lib/src/ai-http-client.ts)
  - Differentiates response errors, request errors, and setup errors
  - Logs status codes and response bodies on HTTP error

----------------------------------------------------------------

9) Configuration and Environment

- Required environment variables (provider tokens):
  - OPENAI_API_KEY
  - GOOGLEAI_API_KEY
  - ANTHROPIC_API_KEY

- dotenv usage:
  - Each agent’s main.ts calls dotenv.config()
  - Note: dotenv is currently not listed in package.json dependencies. Install it to avoid runtime import errors.

  Install command:
  - npm install dotenv --save

- Ports:
  - Defaults per main.ts: OpenAI 4000, GoogleAI 5000, Anthropic 6000
  - Override with PORT environment variable per process

----------------------------------------------------------------

10) Known Issues, Risks, and Suggested Improvements

- Dependency gap: dotenv
  - Files: openai-agent/src/main.ts, googleai-agent/src/main.ts, anthropic-agent/src/main.ts import dotenv, but package.json lacks "dotenv".
  - Fix: npm install dotenv --save

- Bug: Duplicate/incorrect import in anthropic-agent/src/routes.ts
  - Current:
    - Duplicate import lines and a misspelled identifier: "AnthopicRepository" vs "AnthropicRepository"
  - Suggested correction:
    - Use a single import: import AnthropicRepository from './repository/anthropic-repository';

- SQL injection risk in BaseRepository.save (lib/src/base-repository.ts)
  - Raw string interpolation is used to build INSERT statements
  - Chunk only escapes single quotes for 'message' and 'system_instruction'
  - Suggested fix:
    - Use parameterized queries with placeholders (?) and bound parameters for all tables and fields

- Global axios header mutation (lib/src/ai-http-client.ts)
  - axios.defaults.headers.common is mutated before each call; this is global state
  - Suggested fix:
    - Pass headers via axios.post(url, body, { headers }) per request to avoid cross-request contamination

- NaN handling for temperature/top_p in router
  - Files: lib/src/agent-router.ts
  - parseFloat() may return NaN if DB value is empty; better to default/fallback
  - Suggested fix:
    - Validate and provide defaults before passing into body constructors

- setTimeout delay in agent-router
  - A 1s artificial delay precedes provider calls when triggering the query
  - Review whether this is intended; otherwise remove to improve responsiveness

- MultiTurnChat.appendUniquely (lib/src/multi-turn-chat.ts)
  - Uses reference equality to dedupe; most sqlite row objects are distinct references even for identical content, so dedupe might not work as intended
  - Suggested improvement:
    - Deduplicate based on id or value equality

- tsconfig path mapping
  - tsconfig.base.json includes "@code-ai-agent/*": ["libs/*"] which points to a non-existent "libs" directory; could be removed for clarity

- Anthropic body ignores temperature/topP
  - anthopic-agent/src/model/anthropic-body.ts methods are no-ops
  - If needed, map to Anthropic parameters (e.g., temperature, top_p) per API semantics

- GoogleAI endpoint and shapes
  - Uses v1beta generateContent; keep API changes in mind (safety settings thresholds/constants may evolve)

----------------------------------------------------------------

11) Extending the System for a New Provider

To add a new provider "foo":

- Create FooBody (src/model/foo-body.ts)
  - Implement the same interface used by routers:
    - setModelToUse, setSystemInstruction, setMultiTurnChat, setTemperature, setTopP, getBody

- Create FooAIHttpClient (src/ai-http-client.ts)
  - Subclass AIHttpClient with provider = 'foo'
  - Implement initClient() to set base URL and headers (token names, version headers, etc.)

- Create FooRepository (src/repository/foo-repository.ts)
  - Extend BaseRepository using a unique DB name (e.g., 'foo')

- Create routes.ts
  - Export const router = createAgentRouter(new FooRepository(), 'foo', FooAIHttpClient, FooBody)

- Create main.ts
  - Express + dotenv.config() + use(router) + listen

- Create project.json
  - Copy from existing agent project.json and adjust paths and ports

----------------------------------------------------------------

12) API Usage Examples (cURL)

- Set model and system instruction (OpenAI)
  curl -X POST http://localhost:4000/openai \
    -H 'Content-Type: application/json' \
    -d '{"model_to_use":"gpt-4o","system_instruction":"You are a helpful assistant"}'

- Add a user message
  curl -X POST http://localhost:4000/openai \
    -H 'Content-Type: application/json' \
    -d '{"role":"user","content":"Hello!"}'

- Trigger the model call (empty body)
  curl -X POST http://localhost:4000/openai \
    -H 'Content-Type: application/json' \
    -d '{}'

- Clear state
  curl -X GET http://localhost:4000/openai/clear

- Replace port and path for GoogleAI and Anthropic accordingly (5000/googleai, 6000/anthropic)

----------------------------------------------------------------

13) Testing

- Jest setup per project with ts-jest; current tests are minimal
  - lib/src/lib/lib.spec.ts validates a trivial lib() function
- To run tests (example):
  - npx nx test lib
  - Extend test coverage for repository logic, agent-router, and body constructors

----------------------------------------------------------------

14) Security and Operational Considerations

- API keys are loaded from environment; ensure secrets management in production
- sqlite databases are per-agent under /tmp; consider persistence and cleanup policies
- Ensure CORS and auth layers if agents are exposed beyond trusted networks
- Logging includes provider error messages; avoid leaking sensitive data in logs

----------------------------------------------------------------

15) Version References

- Nx: ^20.6.0
- TypeScript: ~5.5.2
- Node types: ~18.16.9
- express: ~4.18.1
- axios: ^1.7.8
- sqlite3: ^5.1.7
- jest + ts-jest for tests

----------------------------------------------------------------

Appendix: File Index (Key Files)

- Shared library:
  - lib/src/index.ts
  - lib/src/ai-http-client.ts
  - lib/src/agent-router.ts
  - lib/src/base-repository.ts
  - lib/src/chunk.ts
  - lib/src/multi-turn-chat.ts
  - lib/src/get-disabled-model-response.ts
  - lib/src/get-openai-error-response.ts
  - lib/src/get-googleai-error-response.ts
  - lib/src/get-anthropic-error-response.ts

- OpenAI agent:
  - openai-agent/src/main.ts
  - openai-agent/src/routes.ts
  - openai-agent/src/ai-http-client.ts
  - openai-agent/src/model/openai-body.ts
  - openai-agent/src/repository/openai-repository.ts

- GoogleAI agent:
  - googleai-agent/src/main.ts
  - googleai-agent/src/routes.ts
  - googleai-agent/src/ai-http-client.ts
  - googleai-agent/src/model/googleai-body.ts
  - googleai-agent/src/repository/googleai-repository.ts

- Anthropic agent:
  - anthropic-agent/src/main.ts
  - anthropic-agent/src/routes.ts
  - anthropic-agent/src/ai-http-client.ts
  - anthropic-agent/src/model/anthropic-body.ts
  - anthropic-agent/src/repository/anthropic-repository.ts
```

Explanation of changes:
- I create a single documentation file docs/TECHNICAL_DESCRIPTION.md that comprehensively explains the system without altering existing source code. It also highlights concrete improvement opportunities with exact file references and includes the command needed to install the missing dotenv package.

Files created:
- docs/TECHNICAL_DESCRIPTION.md

Required package to install:
- npm install dotenv --save

OpenAI gpt-5 (13k in, 7k out)


