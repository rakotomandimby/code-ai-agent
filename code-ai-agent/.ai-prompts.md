# Make the LLM API keys to be obtained fron the clients. 

Currently, I must set the LLM API keys as environment variables:

- `OPENAI_API_KEY` - OpenAI API authentication
- `GOOGLEAI_API_KEY` - Google AI API authentication  
- `ANTHROPIC_API_KEY` - Anthropic API authentication

I dont like that. I want the clients to provide the API keys in the requests body.

I  want to handle the API key as a special piece of data that is held in memory at the router level.

Here is the proposed flow:

1.  **Modify `createAgentRouter`**: Introduce a variable within the router's scope (e.g., `let apiKey: string | null = null;`) to store the key in memory.
2.  **Capture the Key**: When a POST request arrives, check if it contains an `api_key` property. If it does, store its value in the `apiKey` variable and do not create a `Chunk` for it.
3.  **Pass the Key to the HTTP Client**: Inside the `setTimeout` block, when you create the `aiHttpClient` instance, pass the stored `apiKey` to it through a new method (e.g., `aiHttpClient.setApiKey(apiKey)`).
4.  **Modify `AIHttpClient`**:
    *   Add a new `setApiKey(key: string)` method.
    *   Modify the logic so that if an API key is provided via this method, it is used for authentication. If not, it fails with a clear error message and the expected response structure.

This approach keeps the API key in memory only for the duration of the request cycle. It respects the core architecture for handling conversational state while treating sensitive authentication data with the appropriate care.

Then tell me the structure of the body of the POST request that the clients must send to the agents.

Tell me the  full new content of the files I need to modify to implement this change.
